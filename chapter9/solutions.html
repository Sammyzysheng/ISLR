<!DOCTYPE HTML>
<html lang="en" >
    
    <head>
        
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <title>Solutions | An Introduction to Statistical Learning:</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="generator" content="GitBook 2.2.0">
        
        
        <meta name="HandheldFriendly" content="true"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black">
        <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
        <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">
        
    <link rel="stylesheet" href="../gitbook/style.css">
    
    
        <link rel="stylesheet" href="../styles/website.css">
    

        
    
    
    <link rel="next" href="../chapter10/index.html" />
    
    
    <link rel="prev" href="../chapter9/lab.html" />
    

        
    </head>
    <body>
        
        
    <div class="book" data-level="8.2" data-basepath=".." data-revision="Wed Aug 19 2015 19:44:06 GMT+0100 (BST)">
    

<div class="book-summary">
    <div class="book-search">
        <input type="text" placeholder="Type to search" class="form-control" />
    </div>
    <ul class="summary">
        
        
        
        

        

        
    
        <li class="chapter " data-level="0" data-path="index.html">
            
                
                    <a href="../index.html">
                
                        <i class="fa fa-check"></i>
                        
                        Introduction
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1" data-path="chapter2/index.html">
            
                
                    <a href="../chapter2/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.</b>
                        
                        Chapter 2. Statistical Learning
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.1" data-path="chapter2/lab.html">
            
                
                    <a href="../chapter2/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="chapter2/solutions.html">
            
                
                    <a href="../chapter2/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>1.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2" data-path="chapter3/index.html">
            
                
                    <a href="../chapter3/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.</b>
                        
                        Chapter 3. Linear Regression
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1" data-path="chapter3/lab.html">
            
                
                    <a href="../chapter3/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="chapter3/solutions.html">
            
                
                    <a href="../chapter3/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>2.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="3" data-path="chapter4/index.html">
            
                
                    <a href="../chapter4/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.</b>
                        
                        Chapter 4. Classification
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.1" data-path="chapter4/lab.html">
            
                
                    <a href="../chapter4/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="chapter4/solutions.html">
            
                
                    <a href="../chapter4/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>3.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="4" data-path="chapter5/index.html">
            
                
                    <a href="../chapter5/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.</b>
                        
                        Chapter 5. Resampling Methods
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.1" data-path="chapter5/lab.html">
            
                
                    <a href="../chapter5/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="chapter5/solutions.html">
            
                
                    <a href="../chapter5/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>4.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="5" data-path="chapter6/index.html">
            
                
                    <a href="../chapter6/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>5.</b>
                        
                        Chapter 6. Linear Model Selection and Regularization
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="5.1" data-path="chapter6/lab.html">
            
                
                    <a href="../chapter6/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>5.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="chapter6/solutions.html">
            
                
                    <a href="../chapter6/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>5.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="6" data-path="chapter7/index.html">
            
                
                    <a href="../chapter7/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>6.</b>
                        
                        Chapter 7. Moving Beyond Linearity
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="6.1" data-path="chapter7/lab.html">
            
                
                    <a href="../chapter7/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>6.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="6.2" data-path="chapter7/solutions.html">
            
                
                    <a href="../chapter7/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>6.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="7" data-path="chapter8/index.html">
            
                
                    <a href="../chapter8/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>7.</b>
                        
                        Chapter 8. Tree-Based Methods
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="7.1" data-path="chapter8/lab.html">
            
                
                    <a href="../chapter8/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>7.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="7.2" data-path="chapter8/solutions.html">
            
                
                    <a href="../chapter8/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>7.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="8" data-path="chapter9/index.html">
            
                
                    <a href="../chapter9/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>8.</b>
                        
                        Chapter 9. Support Vector Machines
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="8.1" data-path="chapter9/lab.html">
            
                
                    <a href="../chapter9/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>8.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter active" data-level="8.2" data-path="chapter9/solutions.html">
            
                
                    <a href="../chapter9/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>8.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="9" data-path="chapter10/index.html">
            
                
                    <a href="../chapter10/index.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>9.</b>
                        
                        Chapter 10. Unsupervised Learning
                    </a>
            
            
            <ul class="articles">
                
    
        <li class="chapter " data-level="9.1" data-path="chapter10/lab.html">
            
                
                    <a href="../chapter10/lab.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>9.1.</b>
                        
                        Lab
                    </a>
            
            
        </li>
    
        <li class="chapter " data-level="9.2" data-path="chapter10/solutions.html">
            
                
                    <a href="../chapter10/solutions.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>9.2.</b>
                        
                        Solutions
                    </a>
            
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="10" data-path="references.html">
            
                
                    <a href="../references.html">
                
                        <i class="fa fa-check"></i>
                        
                            <b>10.</b>
                        
                        References
                    </a>
            
            
        </li>
    


        
        <li class="divider"></li>
        <li>
            <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
                Published with GitBook
            </a>
        </li>
        
    </ul>
</div>

    <div class="book-body">
        <div class="body-inner">
            <div class="book-header">
    <!-- Actions Left -->
    <a href="#" class="btn pull-left toggle-summary" aria-label="Table of Contents"><i class="fa fa-align-justify"></i></a>
    <a href="#" class="btn pull-left toggle-search" aria-label="Search"><i class="fa fa-search"></i></a>
    
    <div id="font-settings-wrapper" class="dropdown pull-left">
        <a href="#" class="btn toggle-dropdown" aria-label="Font Settings"><i class="fa fa-font"></i>
        </a>
        <div class="dropdown-menu font-settings">
    <div class="dropdown-caret">
        <span class="caret-outer"></span>
        <span class="caret-inner"></span>
    </div>

    <div class="buttons">
        <button type="button" id="reduce-font-size" class="button size-2">A</button>
        <button type="button" id="enlarge-font-size" class="button size-2">A</button>
    </div>

    <div class="buttons font-family-list">
        <button type="button" data-font="0" class="button">Serif</button>
        <button type="button" data-font="1" class="button">Sans</button>
    </div>

    <div class="buttons color-theme-list">
        <button type="button" id="color-theme-preview-0" class="button size-3" data-theme="0">White</button>
        <button type="button" id="color-theme-preview-1" class="button size-3" data-theme="1">Sepia</button>
        <button type="button" id="color-theme-preview-2" class="button size-3" data-theme="2">Night</button>
    </div>
</div>

    </div>

    <!-- Actions Right -->
    
    <div class="dropdown pull-right">
        <a href="#" class="btn toggle-dropdown" aria-label="Share"><i class="fa fa-share-alt"></i>
        </a>
        <div class="dropdown-menu font-settings dropdown-left">
            <div class="dropdown-caret">
                <span class="caret-outer"></span>
                <span class="caret-inner"></span>
            </div>
            <div class="buttons">
                <button type="button" data-sharing="twitter" class="button">
                    Share on Twitter
                </button>
                <button type="button" data-sharing="google-plus" class="button">
                    Share on Google
                </button>
                <button type="button" data-sharing="facebook" class="button">
                    Share on Facebook
                </button>
                <button type="button" data-sharing="weibo" class="button">
                    Share on Weibo
                </button>
                <button type="button" data-sharing="instapaper" class="button">
                    Share on Instapaper
                </button>
            </div>
        </div>
    </div>
    

    
    <a href="#" target="_blank" class="btn pull-right google-plus-sharing-link sharing-link" data-sharing="google-plus" aria-label="Google"><i class="fa fa-google-plus"></i></a>
    
    
    <a href="#" target="_blank" class="btn pull-right facebook-sharing-link sharing-link" data-sharing="facebook" aria-label="Facebook"><i class="fa fa-facebook"></i></a>
    
    
    <a href="#" target="_blank" class="btn pull-right twitter-sharing-link sharing-link" data-sharing="twitter" aria-label="Twitter"><i class="fa fa-twitter"></i></a>
    
    
    


    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="../" >An Introduction to Statistical Learning:</a>
    </h1>
</div>

            <div class="page-wrapper" tabindex="-1">
                <div class="page-inner">
                
                
                    <section class="normal" id="section-">
                    
                        <h2 id="9-7-exercises">9.7 Exercises</h2>
<h3 id="exercise-4">Exercise 4</h3>
<pre><code class="lang-r"><span class="hljs-keyword">library</span>(ISLR)
<span class="hljs-keyword">library</span>(e1071)

set.seed(<span class="hljs-number">0</span>)

DF &lt;- data.frame(x1 = c(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>), x2 = c(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>), y = as.factor(c(rep(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>), rep(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>))))
colors &lt;- c(rep(<span class="hljs-string">&quot;red&quot;</span>, <span class="hljs-number">4</span>), rep(<span class="hljs-string">&quot;blue&quot;</span>, <span class="hljs-number">3</span>))

<span class="hljs-comment"># Use the svm code to find the linear boundary (take the cost of a margin violation to be very large):</span>
svm.fit &lt;- svm(y ~ ., data = DF, kernel = <span class="hljs-string">&quot;linear&quot;</span>, cost = <span class="hljs-number">1e+06</span>, scale = <span class="hljs-literal">FALSE</span>)
print(summary(svm.fit))
</code></pre>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = DF, kernel = &quot;linear&quot;, cost = 1e+06, 
##     scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1e+06 
##       gamma:  0.5 
## 
## Number of Support Vectors:  3
## 
##  ( 2 1 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  0 1
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Use the output from svm.fit to compute/extract the linear decision boundary: For some reason this did not work ... not sure why.</span>
<span class="hljs-comment"># If anyone knows please contact me.  Its based on the decomposition: f(x) = beta_0 + \sum_{i \in S} \alpha_i &lt; x, x_i &gt; given in</span>
<span class="hljs-comment"># the text and using the results from the svm call to get the support vectors via &apos;index&apos;.</span>
<span class="hljs-keyword">if</span> (<span class="hljs-literal">FALSE</span>) {
  beta_0 &lt;- svm.fit$coef0
  beta_x1 &lt;- sum(svm.fit$coefs * DF[svm.fit$index, ]$x1)
  beta_x2 &lt;- sum(svm.fit$coefs * DF[svm.fit$index, ]$x2)

  <span class="hljs-comment"># Plot the decision boundary with abline( a=-beta_0/beta_x1, b=-beta_x2/beta_x1 )</span>
}

<span class="hljs-comment"># From a plot of the points the decision boundary is given by the line with:</span>
slope &lt;- (<span class="hljs-number">3.5</span> - <span class="hljs-number">1.5</span>)/(<span class="hljs-number">4</span> - <span class="hljs-number">2</span>)
intercept &lt;- -<span class="hljs-number">2</span> * slope + <span class="hljs-number">1.5</span>

<span class="hljs-comment"># Compute the two margin lines:</span>
slope_m_upper &lt;- slope
intercept_m_upper &lt;- -<span class="hljs-number">2</span> * slope_m_upper + <span class="hljs-number">2</span>

slope_m_lower &lt;- slope
intercept_m_lower &lt;- -<span class="hljs-number">2</span> * slope_m_lower + <span class="hljs-number">1</span>

plot(DF$x1, DF$x2, col = colors, pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">2</span>, xlab = <span class="hljs-string">&quot;X_1&quot;</span>, ylab = <span class="hljs-string">&quot;X_2&quot;</span>, main = <span class="hljs-string">&quot;&quot;</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-1-1.png" alt=""></p>
<pre><code class="lang-r">plot(DF$x1, DF$x2, col = colors, pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">2</span>, xlab = <span class="hljs-string">&quot;X_1&quot;</span>, ylab = <span class="hljs-string">&quot;X_2&quot;</span>, main = <span class="hljs-string">&quot;&quot;</span>)
abline(a = intercept, b = slope, col = <span class="hljs-string">&quot;black&quot;</span>)
abline(a = intercept_m_upper, b = slope_m_upper, col = <span class="hljs-string">&quot;gray&quot;</span>, lty = <span class="hljs-number">2</span>, lwd = <span class="hljs-number">2</span>)
abline(a = intercept_m_lower, b = slope_m_lower, col = <span class="hljs-string">&quot;gray&quot;</span>, lty = <span class="hljs-number">2</span>, lwd = <span class="hljs-number">2</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-1-2.png" alt=""></p>
<h3 id="exercise-5">Exercise 5</h3>
<pre><code class="lang-r"><span class="hljs-keyword">library</span>(MASS)

set.seed(<span class="hljs-number">0</span>)

<span class="hljs-comment"># Part (a):</span>
n &lt;- <span class="hljs-number">5000</span>
p &lt;- <span class="hljs-number">2</span>

x1 &lt;- runif(n) - <span class="hljs-number">0.5</span>
x2 &lt;- runif(n) - <span class="hljs-number">0.5</span>
y &lt;- <span class="hljs-number">1</span> * (x1^<span class="hljs-number">2</span> - x2^<span class="hljs-number">2</span> &gt; <span class="hljs-number">0</span>)
<span class="hljs-comment"># y = 1*( x1^2 + x2^2 &gt; 1 ) y = 1*( abs(x1) - abs(x2) &gt; 0 )</span>
DF &lt;- data.frame(x1 = x1, x2 = x2, y = as.factor(y))

<span class="hljs-comment"># Part (b):</span>
plot(x1, x2, col = (y + <span class="hljs-number">1</span>), pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">1.05</span>, xlab = <span class="hljs-string">&quot;x1&quot;</span>, ylab = <span class="hljs-string">&quot;x2&quot;</span>, main = <span class="hljs-string">&quot;initial data&quot;</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-2-1.png" alt=""></p>
<pre><code class="lang-r"><span class="hljs-comment"># Part (c):</span>
m &lt;- glm(y ~ x1 + x2, data = DF, family = binomial)

<span class="hljs-comment"># Part (d): Predict the class label of the training data.  When type=&apos;response&apos; in predict we output probabilities:</span>
y_hat &lt;- predict(m, newdata = data.frame(x1 = x1, x2 = x2), type = <span class="hljs-string">&quot;response&quot;</span>)
predicted_class &lt;- <span class="hljs-number">1</span> * (y_hat &gt; <span class="hljs-number">0.5</span>)
print(sprintf(<span class="hljs-string">&quot;Linear logistic regression training error rate= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(predicted_class == y)/length(y)))
</code></pre>
<pre><code>## [1] &quot;Linear logistic regression training error rate=   0.395800&quot;
</code></pre><pre><code class="lang-r">plot(x1, x2, col = (predicted_class + <span class="hljs-number">1</span>), pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">1.05</span>, xlab = <span class="hljs-string">&quot;x1&quot;</span>, ylab = <span class="hljs-string">&quot;x2&quot;</span>, main = <span class="hljs-string">&quot;logistic regression: y ~ x1 + x2&quot;</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-2-2.png" alt=""></p>
<pre><code class="lang-r"><span class="hljs-comment"># Part (e-f): Using logistic regression to fit a nonlinear model:</span>
m &lt;- glm(y ~ x1 + x2 + I(x1^<span class="hljs-number">2</span>) + I(x2^<span class="hljs-number">2</span>) + I(x1 * x2), data = DF, family = <span class="hljs-string">&quot;binomial&quot;</span>)
</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(m, newdata = data.frame(x1 = x1, x2 = x2), type = <span class="hljs-string">&quot;response&quot;</span>)
predicted_class &lt;- <span class="hljs-number">1</span> * (y_hat &gt; <span class="hljs-number">0.5</span>)
print(sprintf(<span class="hljs-string">&quot;Non-linear logistic regression training error rate= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(predicted_class == y)/length(y)))
</code></pre>
<pre><code>## [1] &quot;Non-linear logistic regression training error rate=   0.035800&quot;
</code></pre><pre><code class="lang-r">plot(x1, x2, col = (predicted_class + <span class="hljs-number">1</span>), pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">1.05</span>, xlab = <span class="hljs-string">&quot;x1&quot;</span>, ylab = <span class="hljs-string">&quot;x2&quot;</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-2-3.png" alt=""></p>
<pre><code class="lang-r"><span class="hljs-comment"># Part (g): Fit a linear SVM to the data and report the error rate:</span>
dat &lt;- data.frame(x1 = x1, x2 = x2, y = as.factor(y))

<span class="hljs-comment"># Do CV to select the value of cost using the &apos;tune&apos; function:</span>
tune.out &lt;- tune(svm, y ~ ., data = dat, kernel = <span class="hljs-string">&quot;linear&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##   0.1
## 
## - best performance: 0.4888 
## 
## - Detailed performance results:
##    cost  error dispersion
## 1 1e-03 0.4904 0.01743050
## 2 1e-02 0.4904 0.01743050
## 3 1e-01 0.4888 0.02040044
## 4 1e+00 0.4888 0.02040044
## 5 5e+00 0.4888 0.02040044
## 6 1e+01 0.4888 0.02040044
## 7 1e+02 0.4888 0.02040044
## 8 1e+03 0.4888 0.02040044
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># The best model is stored in &apos;tune.out$best.model&apos;</span>
print(tune.out$best.model)
</code></pre>
<pre><code>## 
## Call:
## best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, 
##     0.01, 0.1, 1, 5, 10, 100, 1000)), kernel = &quot;linear&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  0.1 
##       gamma:  0.5 
## 
## Number of Support Vectors:  4913
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(tune.out$best.model, newdata = data.frame(x1 = x1, x2 = x2))
y_hat &lt;- as.numeric(as.character(y_hat))  <span class="hljs-comment"># convert factor responses into numerical values </span>
print(sprintf(<span class="hljs-string">&quot;Linear SVM training error rate= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == y)/length(y)))
</code></pre>
<pre><code>## [1] &quot;Linear SVM training error rate=   0.490400&quot;
</code></pre><pre><code class="lang-r">plot(x1, x2, col = (y_hat + <span class="hljs-number">1</span>), pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">1.05</span>, xlab = <span class="hljs-string">&quot;x1&quot;</span>, ylab = <span class="hljs-string">&quot;x2&quot;</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-2-4.png" alt=""></p>
<pre><code class="lang-r"><span class="hljs-comment"># Part (h-i): Fit a linear SVM to the data and report the error rate:</span>

<span class="hljs-comment"># Do CV to select the value of cost using the &apos;tune&apos; function:</span>
tune.out &lt;- tune(svm, y ~ ., data = dat, kernel = <span class="hljs-string">&quot;radial&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>), gamma = c(<span class="hljs-number">0.5</span>, 
    <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost gamma
##  1000     1
## 
## - best performance: 0.0022 
## 
## - Detailed performance results:
##     cost gamma  error  dispersion
## 1  1e-03   0.5 0.4904 0.019749543
## 2  1e-02   0.5 0.0296 0.013945927
## 3  1e-01   0.5 0.0218 0.011331372
## 4  1e+00   0.5 0.0172 0.006941021
## 5  5e+00   0.5 0.0066 0.004221637
## 6  1e+01   0.5 0.0072 0.004638007
## 7  1e+02   0.5 0.0048 0.003425395
## 8  1e+03   0.5 0.0034 0.002503331
## 9  1e-03   1.0 0.4904 0.019749543
## 10 1e-02   1.0 0.0256 0.013817863
## 11 1e-01   1.0 0.0226 0.008275801
## 12 1e+00   1.0 0.0146 0.005891614
## 13 5e+00   1.0 0.0080 0.003887301
## 14 1e+01   1.0 0.0074 0.004427189
## 15 1e+02   1.0 0.0050 0.003559026
## 16 1e+03   1.0 0.0022 0.001751190
## 17 1e-03   2.0 0.4904 0.019749543
## 18 1e-02   2.0 0.0218 0.011053406
## 19 1e-01   2.0 0.0188 0.007671013
## 20 1e+00   2.0 0.0124 0.003747592
## 21 5e+00   2.0 0.0098 0.004756282
## 22 1e+01   2.0 0.0092 0.002529822
## 23 1e+02   2.0 0.0054 0.003272783
## 24 1e+03   2.0 0.0030 0.002538591
## 25 1e-03   3.0 0.4904 0.019749543
## 26 1e-02   3.0 0.0186 0.011471704
## 27 1e-01   3.0 0.0200 0.007944250
## 28 1e+00   3.0 0.0122 0.004263541
## 29 5e+00   3.0 0.0090 0.004642796
## 30 1e+01   3.0 0.0088 0.002859681
## 31 1e+02   3.0 0.0054 0.003272783
## 32 1e+03   3.0 0.0034 0.002988868
## 33 1e-03   4.0 0.4904 0.019749543
## 34 1e-02   4.0 0.0182 0.008766096
## 35 1e-01   4.0 0.0178 0.007568942
## 36 1e+00   4.0 0.0118 0.003326660
## 37 5e+00   4.0 0.0100 0.004714045
## 38 1e+01   4.0 0.0094 0.002503331
## 39 1e+02   4.0 0.0046 0.003533962
## 40 1e+03   4.0 0.0040 0.002828427
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># The best model is stored in &apos;tune.out$best.model&apos;</span>
print(tune.out$best.model)
</code></pre>
<pre><code>## 
## Call:
## best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, 
##     0.01, 0.1, 1, 5, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 
##     4)), kernel = &quot;radial&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1000 
##       gamma:  1 
## 
## Number of Support Vectors:  91
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(tune.out$best.model, newdata = data.frame(x1 = x1, x2 = x2))
y_hat &lt;- as.numeric(as.character(y_hat))  <span class="hljs-comment"># convert factor responses into numerical values </span>
print(sprintf(<span class="hljs-string">&quot;Nonlinear SVM training error rate= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == y)/length(y)))
</code></pre>
<pre><code>## [1] &quot;Nonlinear SVM training error rate=   0.001400&quot;
</code></pre><pre><code class="lang-r">plot(x1, x2, col = (y_hat + <span class="hljs-number">1</span>), pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">1.05</span>, xlab = <span class="hljs-string">&quot;x1&quot;</span>, ylab = <span class="hljs-string">&quot;x2&quot;</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-2-5.png" alt=""></p>
<h3 id="exercise-6">Exercise 6</h3>
<pre><code class="lang-r">set.seed(<span class="hljs-number">1</span>)

x &lt;- matrix(rnorm(<span class="hljs-number">20</span> * <span class="hljs-number">2</span>), ncol = <span class="hljs-number">2</span>)
y &lt;- c(rep(-<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), rep(+<span class="hljs-number">1</span>, <span class="hljs-number">10</span>))
x[y == <span class="hljs-number">1</span>, ] &lt;- x[y == <span class="hljs-number">1</span>, ] + <span class="hljs-number">1</span>
x[y == <span class="hljs-number">1</span>, ] &lt;- x[y == <span class="hljs-number">1</span>, ] + <span class="hljs-number">0.5</span>

plot(x[, <span class="hljs-number">1</span>], x[, <span class="hljs-number">2</span>], col = (y + <span class="hljs-number">3</span>), pch = <span class="hljs-number">19</span>, cex = <span class="hljs-number">1.25</span>, xlab = <span class="hljs-string">&quot;x1&quot;</span>, ylab = <span class="hljs-string">&quot;x2&quot;</span>, main = <span class="hljs-string">&quot;initial data&quot;</span>)
grid()
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-3-1.png" alt=""></p>
<pre><code class="lang-r">dat &lt;- data.frame(x1 = x[, <span class="hljs-number">1</span>], x2 = x[, <span class="hljs-number">2</span>], y = as.factor(y))
tune.out &lt;- tune(svm, y ~ ., data = dat, kernel = <span class="hljs-string">&quot;linear&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##   0.1
## 
## - best performance: 0.05 
## 
## - Detailed performance results:
##    cost error dispersion
## 1 1e-03  0.45  0.4972145
## 2 1e-02  0.45  0.4972145
## 3 1e-01  0.05  0.1581139
## 4 1e+00  0.05  0.1581139
## 5 5e+00  0.10  0.2108185
## 6 1e+01  0.10  0.2108185
## 7 1e+02  0.10  0.2108185
## 8 1e+03  0.10  0.2108185
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Generate test data:</span>
x_test &lt;- matrix(rnorm(<span class="hljs-number">20</span> * <span class="hljs-number">2</span>), ncol = <span class="hljs-number">2</span>)
y_test &lt;- c(rep(-<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), rep(+<span class="hljs-number">1</span>, <span class="hljs-number">10</span>))
x_test[y_test == <span class="hljs-number">1</span>, ] &lt;- x_test[y_test == <span class="hljs-number">1</span>, ] + <span class="hljs-number">1</span>
x_test[y_test == <span class="hljs-number">1</span>, ] &lt;- x_test[y_test == <span class="hljs-number">1</span>, ] + <span class="hljs-number">0.5</span>

dat_test &lt;- data.frame(x1 = x_test[, <span class="hljs-number">1</span>], x2 = x_test[, <span class="hljs-number">2</span>], y = as.factor(y_test))

y_hat &lt;- predict(tune.out$best.model, newdata = dat_test)
y_hat &lt;- as.numeric(as.character(y_hat))  <span class="hljs-comment"># convert factor responses into numerical values </span>
print(sprintf(<span class="hljs-string">&quot;Linear SVM test error rate= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == y)/length(y)))
</code></pre>
<pre><code>## [1] &quot;Linear SVM test error rate=   0.100000&quot;
</code></pre><h3 id="exercise-7">Exercise 7</h3>
<pre><code class="lang-r">set.seed(<span class="hljs-number">0</span>)

Auto &lt;- read.csv(<span class="hljs-string">&quot;http://www-bcf.usc.edu/~gareth/ISL/Auto.csv&quot;</span>, header = <span class="hljs-literal">T</span>, na.strings = <span class="hljs-string">&quot;?&quot;</span>)
Auto &lt;- na.omit(Auto)
Auto$name &lt;- <span class="hljs-literal">NULL</span>

<span class="hljs-comment"># Part (a):</span>
<span class="hljs-keyword">if</span> (<span class="hljs-literal">TRUE</span>) {
    AbvMedian &lt;- rep(<span class="hljs-number">0</span>, dim(Auto)[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 0 =&gt; less than the median of mpg </span>
    AbvMedian[Auto$mpg &gt; median(Auto$mpg)] &lt;- <span class="hljs-number">1</span>  <span class="hljs-comment"># 1 =&gt; greater than the median of mpg </span>
} <span class="hljs-keyword">else</span> {
    AbvMedian &lt;- rep(<span class="hljs-string">&quot;LT&quot;</span>, dim(Auto)[<span class="hljs-number">1</span>])
    AbvMedian[Auto$mpg &gt; median(Auto$mpg)] &lt;- <span class="hljs-string">&quot;GT&quot;</span>
}
AbvMedian &lt;- as.factor(AbvMedian)
Auto$AbvMedian &lt;- AbvMedian
Auto$mpg &lt;- <span class="hljs-literal">NULL</span>

<span class="hljs-comment"># Part (b):</span>
tune.out &lt;- tune(svm, AbvMedian ~ ., data = Auto, kernel = <span class="hljs-string">&quot;linear&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##  0.01
## 
## - best performance: 0.09198718 
## 
## - Detailed performance results:
##    cost      error dispersion
## 1 1e-03 0.13301282 0.06623742
## 2 1e-02 0.09198718 0.05314051
## 3 1e-01 0.09705128 0.05251387
## 4 1e+00 0.09980769 0.06106182
## 5 5e+00 0.10493590 0.06456815
## 6 1e+01 0.10493590 0.06456815
## 7 1e+02 0.10237179 0.06172518
## 8 1e+03 0.10237179 0.06172518
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Some plots to explore with:</span>
plot(tune.out$best.model, Auto, weight ~ horsepower)
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-4-1.png" alt=""></p>
<pre><code class="lang-r">plot(tune.out$best.model, Auto, cylinders ~ year)
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-4-2.png" alt=""></p>
<pre><code class="lang-r">plot(tune.out$best.model, Auto, cylinders ~ horsepower)
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-4-3.png" alt=""></p>
<pre><code class="lang-r"><span class="hljs-comment"># Part (c) radial kernel:</span>
tune.out &lt;- tune(svm, AbvMedian ~ ., data = Auto, kernel = <span class="hljs-string">&quot;radial&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>), gamma = c(<span class="hljs-number">0.5</span>, 
    <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost gamma
##     1     1
## 
## - best performance: 0.07153846 
## 
## - Detailed performance results:
##     cost gamma      error dispersion
## 1  1e-03   0.5 0.56891026 0.04198623
## 2  1e-02   0.5 0.56891026 0.04198623
## 3  1e-01   0.5 0.08935897 0.04884449
## 4  1e+00   0.5 0.07916667 0.05339765
## 5  5e+00   0.5 0.08166667 0.04495840
## 6  1e+01   0.5 0.08160256 0.05091217
## 7  1e+02   0.5 0.10467949 0.05988859
## 8  1e+03   0.5 0.11224359 0.05009346
## 9  1e-03   1.0 0.56891026 0.04198623
## 10 1e-02   1.0 0.56891026 0.04198623
## 11 1e-01   1.0 0.08935897 0.05314219
## 12 1e+00   1.0 0.07153846 0.05389633
## 13 5e+00   1.0 0.08666667 0.03643874
## 14 1e+01   1.0 0.07647436 0.04325754
## 15 1e+02   1.0 0.08923077 0.04540163
## 16 1e+03   1.0 0.08923077 0.04540163
## 17 1e-03   2.0 0.56891026 0.04198623
## 18 1e-02   2.0 0.56891026 0.04198623
## 19 1e-01   2.0 0.11250000 0.08236078
## 20 1e+00   2.0 0.07660256 0.05276112
## 21 5e+00   2.0 0.07910256 0.04084741
## 22 1e+01   2.0 0.08673077 0.03456265
## 23 1e+02   2.0 0.09435897 0.03999530
## 24 1e+03   2.0 0.09435897 0.03999530
## 25 1e-03   3.0 0.56891026 0.04198623
## 26 1e-02   3.0 0.56891026 0.04198623
## 27 1e-01   3.0 0.34467949 0.09233541
## 28 1e+00   3.0 0.08173077 0.05523019
## 29 5e+00   3.0 0.08423077 0.03636273
## 30 1e+01   3.0 0.08423077 0.03636273
## 31 1e+02   3.0 0.08929487 0.03655289
## 32 1e+03   3.0 0.08929487 0.03655289
## 33 1e-03   4.0 0.56891026 0.04198623
## 34 1e-02   4.0 0.56891026 0.04198623
## 35 1e-01   4.0 0.53564103 0.07557472
## 36 1e+00   4.0 0.07910256 0.04905343
## 37 5e+00   4.0 0.07910256 0.03292568
## 38 1e+01   4.0 0.08160256 0.03553445
## 39 1e+02   4.0 0.08673077 0.03837127
## 40 1e+03   4.0 0.08673077 0.03837127
</code></pre><pre><code class="lang-r">plot(tune.out$best.model, Auto, weight ~ horsepower)
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-4-4.png" alt=""></p>
<pre><code class="lang-r">plot(tune.out$best.model, Auto, cylinders ~ year)
</code></pre>
<p><img src="solutions_files/figure-markdown_github+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-4-5.png" alt=""></p>
<pre><code class="lang-r"><span class="hljs-comment"># Part (c) polynomial kernel:</span>
tune.out &lt;- tune(svm, AbvMedian ~ ., data = Auto, kernel = <span class="hljs-string">&quot;polynomial&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>), 
    degree = c(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost degree
##   100      3
## 
## - best performance: 0.07647436 
## 
## - Detailed performance results:
##     cost degree      error dispersion
## 1  1e-03      1 0.54070513 0.02854824
## 2  1e-02      1 0.11487179 0.03687839
## 3  1e-01      1 0.09429487 0.02927966
## 4  1e+00      1 0.09429487 0.03167651
## 5  5e+00      1 0.08166667 0.02907271
## 6  1e+01      1 0.08673077 0.02154213
## 7  1e+02      1 0.08929487 0.02767907
## 8  1e+03      1 0.08673077 0.03238015
## 9  1e-03      2 0.54070513 0.02854824
## 10 1e-02      2 0.41602564 0.09510107
## 11 1e-01      2 0.27288462 0.06169411
## 12 1e+00      2 0.24974359 0.09233499
## 13 5e+00      2 0.18596154 0.04015781
## 14 1e+01      2 0.18352564 0.04860916
## 15 1e+02      2 0.17326923 0.04998543
## 16 1e+03      2 0.16564103 0.06467847
## 17 1e-03      3 0.41346154 0.10154632
## 18 1e-02      3 0.26269231 0.06651432
## 19 1e-01      3 0.19653846 0.08760538
## 20 1e+00      3 0.09929487 0.03400384
## 21 5e+00      3 0.09173077 0.02959606
## 22 1e+01      3 0.07903846 0.02802006
## 23 1e+02      3 0.07647436 0.03377427
## 24 1e+03      3 0.08653846 0.04776394
## 25 1e-03      4 0.45423077 0.06640330
## 26 1e-02      4 0.38012821 0.07805202
## 27 1e-01      4 0.27038462 0.08220294
## 28 1e+00      4 0.19358974 0.07548574
## 29 5e+00      4 0.19365385 0.04717381
## 30 1e+01      4 0.16301282 0.04413317
## 31 1e+02      4 0.14506410 0.05769710
## 32 1e+03      4 0.14775641 0.03661928
## 33 1e-03      5 0.39794872 0.07665480
## 34 1e-02      5 0.28557692 0.08721784
## 35 1e-01      5 0.26269231 0.06651432
## 36 1e+00      5 0.13237179 0.06470603
## 37 5e+00      5 0.13237179 0.05493677
## 38 1e+01      5 0.13243590 0.04357633
## 39 1e+02      5 0.09935897 0.03240545
## 40 1e+03      5 0.08929487 0.03674971
</code></pre><h3 id="exercise-8">Exercise 8</h3>
<pre><code class="lang-r">set.seed(<span class="hljs-number">0</span>)

<span class="hljs-comment"># Part (a):</span>
n &lt;- dim(OJ)[<span class="hljs-number">1</span>]
n_train &lt;- <span class="hljs-number">800</span>
train_inds &lt;- sample(<span class="hljs-number">1</span>:n, n_train)
test_inds &lt;- (<span class="hljs-number">1</span>:n)[-train_inds]
n_test &lt;- length(test_inds)

<span class="hljs-comment"># Part (b) Use a linear kernel to start with:</span>
svm.fit &lt;- svm(Purchase ~ ., data = OJ, kernel = <span class="hljs-string">&quot;linear&quot;</span>, cost = <span class="hljs-number">0.01</span>)
print(summary(svm.fit))
</code></pre>
<pre><code>## 
## Call:
## svm(formula = Purchase ~ ., data = OJ, kernel = &quot;linear&quot;, cost = 0.01)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  0.01 
##       gamma:  0.05555556 
## 
## Number of Support Vectors:  560
## 
##  ( 279 281 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  CH MM
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Part (c) Use this specific SVM to estimate training/testing error rates:</span>
y_hat &lt;- predict(svm.fit, newdata = OJ[train_inds, ])
print(table(predicted = y_hat, truth = OJ[train_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 430  75
##        MM  61 234
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Linear SVM training error rate (cost=0.01)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[train_inds, ]$Purchase)/n_train))
</code></pre>
<pre><code>## [1] &quot;Linear SVM training error rate (cost=0.01)=   0.170000&quot;
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(svm.fit, newdata = OJ[test_inds, ])
print(table(predicted = y_hat, truth = OJ[test_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 146  25
##        MM  16  83
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Linear SVM testing error rate (cost=0.01)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[test_inds, ]$Purchase)/n_test))
</code></pre>
<pre><code>## [1] &quot;Linear SVM testing error rate (cost=0.01)=   0.151852&quot;
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Part (d): Use tune to select an optimal value for cost when we have a linear kernel:</span>
tune.out &lt;- tune(svm, Purchase ~ ., data = OJ, kernel = <span class="hljs-string">&quot;linear&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##     1
## 
## - best performance: 0.1663551 
## 
## - Detailed performance results:
##    cost     error dispersion
## 1 1e-03 0.2355140 0.04174920
## 2 1e-02 0.1728972 0.03531397
## 3 1e-01 0.1710280 0.03116822
## 4 1e+00 0.1663551 0.02636037
## 5 5e+00 0.1672897 0.02836431
## 6 1e+01 0.1700935 0.02405017
## 7 1e+02 0.1700935 0.02445036
## 8 1e+03 0.1747664 0.02415084
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Part (e): Predict the performance on training and testing using the best linear model:</span>
y_hat &lt;- predict(tune.out$best.model, newdata = OJ[train_inds, ])
print(table(predicted = y_hat, truth = OJ[train_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 434  75
##        MM  57 234
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Linear SVM training error rate (optimal cost=1)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[train_inds, ]$Purchase)/n_train))
</code></pre>
<pre><code>## [1] &quot;Linear SVM training error rate (optimal cost=1)=   0.165000&quot;
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(tune.out$best.model, newdata = OJ[test_inds, ])
print(table(predicted = y_hat, truth = OJ[test_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 145  23
##        MM  17  85
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Linear SVM testing error rate (optimal cost=1)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[test_inds, ]$Purchase)/n_test))
</code></pre>
<pre><code>## [1] &quot;Linear SVM testing error rate (optimal cost=1)=   0.148148&quot;
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Part (f): Use a radial kernel:</span>
tune.out &lt;- tune(svm, Purchase ~ ., data = OJ, kernel = <span class="hljs-string">&quot;radial&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>), gamma = c(<span class="hljs-number">0.5</span>, 
    <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost gamma
##     1   0.5
## 
## - best performance: 0.1971963 
## 
## - Detailed performance results:
##     cost gamma     error dispersion
## 1  1e-03   0.5 0.3897196 0.06594509
## 2  1e-02   0.5 0.3897196 0.06594509
## 3  1e-01   0.5 0.2616822 0.04703947
## 4  1e+00   0.5 0.1971963 0.03697860
## 5  5e+00   0.5 0.2158879 0.03481578
## 6  1e+01   0.5 0.2149533 0.03412602
## 7  1e+02   0.5 0.2327103 0.03509343
## 8  1e+03   0.5 0.2345794 0.04588014
## 9  1e-03   1.0 0.3897196 0.06594509
## 10 1e-02   1.0 0.3897196 0.06594509
## 11 1e-01   1.0 0.2981308 0.04875165
## 12 1e+00   1.0 0.2056075 0.03890964
## 13 5e+00   1.0 0.2196262 0.04067777
## 14 1e+01   1.0 0.2224299 0.04198101
## 15 1e+02   1.0 0.2355140 0.04080878
## 16 1e+03   1.0 0.2336449 0.04383566
## 17 1e-03   2.0 0.3897196 0.06594509
## 18 1e-02   2.0 0.3897196 0.06594509
## 19 1e-01   2.0 0.3495327 0.06264704
## 20 1e+00   2.0 0.2224299 0.03349453
## 21 5e+00   2.0 0.2308411 0.04014947
## 22 1e+01   2.0 0.2364486 0.03990702
## 23 1e+02   2.0 0.2429907 0.05555313
## 24 1e+03   2.0 0.2551402 0.05024179
## 25 1e-03   3.0 0.3897196 0.06594509
## 26 1e-02   3.0 0.3897196 0.06594509
## 27 1e-01   3.0 0.3579439 0.06246863
## 28 1e+00   3.0 0.2214953 0.03238977
## 29 5e+00   3.0 0.2299065 0.03945456
## 30 1e+01   3.0 0.2308411 0.03634327
## 31 1e+02   3.0 0.2429907 0.04765439
## 32 1e+03   3.0 0.2551402 0.04472331
## 33 1e-03   4.0 0.3897196 0.06594509
## 34 1e-02   4.0 0.3897196 0.06594509
## 35 1e-01   4.0 0.3700935 0.06581988
## 36 1e+00   4.0 0.2205607 0.03446559
## 37 5e+00   4.0 0.2336449 0.03659608
## 38 1e+01   4.0 0.2308411 0.04134040
## 39 1e+02   4.0 0.2429907 0.03915827
## 40 1e+03   4.0 0.2542056 0.04510146
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(tune.out$best.model, newdata = OJ[train_inds, ])
print(table(predicted = y_hat, truth = OJ[train_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 454  70
##        MM  37 239
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Radial SVM training error rate (optimal)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[train_inds, ]$Purchase)/n_train))
</code></pre>
<pre><code>## [1] &quot;Radial SVM training error rate (optimal)=   0.133750&quot;
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(tune.out$best.model, newdata = OJ[test_inds, ])
print(table(predicted = y_hat, truth = OJ[test_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 150  27
##        MM  12  81
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Radial SVM testing error rate (optimal)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[test_inds, ]$Purchase)/n_test))
</code></pre>
<pre><code>## [1] &quot;Radial SVM testing error rate (optimal)=   0.144444&quot;
</code></pre><pre><code class="lang-r"><span class="hljs-comment"># Part (g): Use a polynomial kernel:</span>
tune.out &lt;- tune(svm, Purchase ~ ., data = OJ, kernel = <span class="hljs-string">&quot;polynomial&quot;</span>, ranges = list(cost = c(<span class="hljs-number">0.001</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>), 
    degree = c(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)))
print(summary(tune.out))  <span class="hljs-comment"># &lt;- use this output to select the optimal cost value</span>
</code></pre>
<pre><code>## 
## Parameter tuning of &apos;svm&apos;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost degree
##     1      1
## 
## - best performance: 0.1700935 
## 
## - Detailed performance results:
##     cost degree     error dispersion
## 1  1e-03      1 0.3897196 0.04134040
## 2  1e-02      1 0.3728972 0.04630126
## 3  1e-01      1 0.1719626 0.03557409
## 4  1e+00      1 0.1700935 0.03784758
## 5  5e+00      1 0.1728972 0.03718796
## 6  1e+01      1 0.1738318 0.03795001
## 7  1e+02      1 0.1728972 0.03821758
## 8  1e+03      1 0.1757009 0.03680761
## 9  1e-03      2 0.3897196 0.04134040
## 10 1e-02      2 0.3691589 0.04891065
## 11 1e-01      2 0.3028037 0.04518745
## 12 1e+00      2 0.1962617 0.02331251
## 13 5e+00      2 0.1850467 0.03170844
## 14 1e+01      2 0.1850467 0.03463412
## 15 1e+02      2 0.1803738 0.02822712
## 16 1e+03      2 0.1803738 0.03116822
## 17 1e-03      3 0.3897196 0.04134040
## 18 1e-02      3 0.3691589 0.04891065
## 19 1e-01      3 0.2672897 0.04988316
## 20 1e+00      3 0.1878505 0.01674726
## 21 5e+00      3 0.1822430 0.01541977
## 22 1e+01      3 0.1859813 0.01354334
## 23 1e+02      3 0.1953271 0.02470702
## 24 1e+03      3 0.2028037 0.02788118
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(tune.out$best.model, newdata = OJ[train_inds, ])
print(table(predicted = y_hat, truth = OJ[train_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 433  74
##        MM  58 235
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Polynomial SVM training error rate (optimal)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[train_inds, ]$Purchase)/n_train))
</code></pre>
<pre><code>## [1] &quot;Polynomial SVM training error rate (optimal)=   0.165000&quot;
</code></pre><pre><code class="lang-r">y_hat &lt;- predict(tune.out$best.model, newdata = OJ[test_inds, ])
print(table(predicted = y_hat, truth = OJ[test_inds, ]$Purchase))
</code></pre>
<pre><code>##          truth
## predicted  CH  MM
##        CH 144  24
##        MM  18  84
</code></pre><pre><code class="lang-r">print(sprintf(<span class="hljs-string">&quot;Polynomial SVM testing error rate (optimal)= %10.6f&quot;</span>, <span class="hljs-number">1</span> - sum(y_hat == OJ[test_inds, ]$Purchase)/n_test))
</code></pre>
<pre><code>## [1] &quot;Polynomial SVM testing error rate (optimal)=   0.155556&quot;
</code></pre>
                    
                    </section>
                
                
                </div>
            </div>
        </div>

        
        <a href="../chapter9/lab.html" class="navigation navigation-prev " aria-label="Previous page: Lab"><i class="fa fa-angle-left"></i></a>
        
        
        <a href="../chapter10/index.html" class="navigation navigation-next " aria-label="Next page: Chapter 10. Unsupervised Learning"><i class="fa fa-angle-right"></i></a>
        
    </div>
</div>

        
<script src="../gitbook/app.js"></script>

<script>
require(["gitbook"], function(gitbook) {
    var config = {"fontSettings":{"theme":null,"family":"sans","size":2}};
    gitbook.start(config);
});
</script>

        
    </body>
    
</html>
